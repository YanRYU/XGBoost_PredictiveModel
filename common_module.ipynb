{"cells":[{"cell_type":"code","source":["import time\nimport datetime\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nimport inspect\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa80c487-d038-4681-ae39-d56f5b70ae6e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import random\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8991b55-3a08-459c-8535-29d8a8d06213"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import datetime as dt\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4aa7ac6-4957-468d-9e58-0048a525be91"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def CountRowNum(df,Column):\n  return df.agg(count(Column).alias(\"cnt_\"+Column),countDistinct(Column).alias(\"dcnt_\"+Column))\ndef CountColNum(df):\n  return len(df.columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74156794-1a95-4080-81b2-a7ed08cc8030"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## for tansformation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12181c1a-60bb-431a-b8ca-137aadf27a81"}}},{"cell_type":"code","source":["def get_dummy(df,id_lst):\n  catg_casewhen_lst = []\n  \n  descri_table = spark.createDataFrame(df.dtypes,['col_name','data_type'])\n  descri_table.where(\"data_type = 'string' \")\n\n  #get column names by datatype\n  catg_column = [i.col_name for i in descri_table.where(\"data_type = 'string' \").collect() if i.col_name not in id_lst]\n  print(\"catg_column: \\n\", catg_column)\n\n  non_catg_column = [i.col_name for i in descri_table.where(\"data_type in ('date','bigint','int','double','float') \").collect() if i.col_name not in id_lst]\n  print(\"non_catg_column: \\n\",non_catg_column)\n\n  # get distinct data content from each column\n  for c in catg_column: \n    distinct_lst = [row[0] for row in df.selectExpr(\n      \"\"\"lower(replace(replace(replace(replace(replace(replace(replace({c},'-','_'),'/','_'),'&','_'),'!','_'),'#','_'),' ','_'),'.','_')) as {c}\"\"\".format(c= c)\n                                                         ).distinct().collect()]\n    # trainsform data into dummy \n    for i in range(len(distinct_lst)):\n      catg_casewhen_lst.append(\"\"\"case when lower(replace(replace(replace(replace(replace(replace({c},'-','_'),'/','_'),'&','_'),'!','_'),'#','_'),' ','_')) = '{x}' then 1 else 0 end as {c}_{x}\"\"\".format(c = c, x = distinct_lst[i]))\n#   return(catg_casewhen_lst)\n\n# produce dummy data \n  df_dummy = df.selectExpr(*id_lst,*non_catg_column,*catg_casewhen_lst)\n  return(df_dummy)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46ef36be-3daa-469c-bafd-e67eaa31bfb8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_cnt_from_catg_data(df,id_lst):\n  catg_casewhen_lst = []\n  \n  descri_table = spark.createDataFrame(df.dtypes,['col_name','data_type'])\n  descri_table.where(\"data_type = 'string' \")\n\n  #get column names by datatype\n  catg_column = [i.col_name for i in descri_table.where(\"data_type = 'string' \").collect() if i.col_name not in id_lst]\n  print(\"catg_column: \\n\", catg_column)\n\n  non_catg_column = [i.col_name for i in descri_table.where(\"data_type in ('date','bigint','int','double','float') \").collect()if i.col_name not in id_lst]\n  print(\"non_catg_column: \\n\",non_catg_column)\n  \n  #print('1')\n  #df_catg_all = df.groupBy(catg_column[0]).agg(countDistinct(col(id_)).alias('id_cnts'))\n  \n  \n  for c in range(len(catg_column)):\n    #print(c)\n    df_catg = df.groupBy(catg_column[c]).agg(countDistinct(col(id_)).alias('id_cnts')) \n    display(df_catg)\n\n  return()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83b9e33e-3a7d-41a1-9904-9c29739079f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#percentile_approx: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.percentile_approx.html\ndef summary_for_numeric_data(df,id_lst):\n  \n  descri_table = spark.createDataFrame(df.dtypes,['col_name','data_type'])\n  descri_table.where(\"data_type = 'string' \")\n\n  #get column names by datatype\n  catg_column = [i.col_name for i in descri_table.where(\"data_type = 'string' \").collect() if i.col_name not in id_lst]\n  print(\"catg_column: \\n\", catg_column)\n\n  non_catg_column = [i.col_name for i in descri_table.where(\"data_type in ('date','bigint','int','double','float') \").collect() if i.col_name not in id_lst]\n  \n  \n  print(\"non_catg_column: \\n\",non_catg_column)\n  \n  #print('1')\n  #df_catg_all = df.groupBy(catg_column[0]).agg(countDistinct(col(id_)).alias('id_cnts'))\n  \n  \n  for c in range(len(non_catg_column)):\n    print(non_catg_column[c],\":\")\n    df_summary = df.agg(round(max(col(non_catg_column[c])),2).alias('max'),\n                        round(min(col(non_catg_column[c])),2).alias('min'),\n                        round(mean(col(non_catg_column[c])),2).alias('mean')\n                       ) \n    # df_summary.withColumn(\"Q1\",lit(winback.select(percentile_approx(\"P6M_AVG_INV_AMT\", 0.25, 1000000).alias('Q1')).collect()[0].Q1))\n    df_summary = df_summary.withColumn(\"Q1\",lit(df.select(percentile_approx(non_catg_column[c], 0.25, 1000000).alias('Q1')).collect()[0].Q1))\n    df_summary = df_summary.withColumn(\"Q2\",lit(df.select(percentile_approx(non_catg_column[c], 0.5, 1000000).alias('Q2')).collect()[0].Q2))\n    df_summary = df_summary.withColumn(\"Q3\",lit(df.select(percentile_approx(non_catg_column[c], 0.75, 1000000).alias('Q3')).collect()[0].Q3))\n    display(df_summary)\n  return()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cad7aa55-17f8-4d6e-80c1-8cfc5f2fbd3d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#percentile_approx: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.percentile_approx.html\ndef select_for_numeric_data(df,id_lst):\n  \n  descri_table = spark.createDataFrame(df.dtypes,['col_name','data_type'])\n  descri_table.where(\"data_type = 'string' \")\n\n  #get column names by datatype\n  catg_column = [i.col_name for i in descri_table.where(\"data_type = 'string' \").collect() if i.col_name not in id_lst]\n  print(\"catg_column: \\n\", catg_column)\n\n  non_catg_column = [i.col_name for i in descri_table.where(\"data_type in ('date','bigint','int','double','float') \").collect() if i.col_name not in id_lst]\n  \n  \n  print(\"non_catg_column: \\n\",non_catg_column)\n  \n  #print('1')\n  #df_catg_all = df.groupBy(catg_column[0]).agg(countDistinct(col(id_)).alias('id_cnts'))\n  \n  \n  for c in range(len(non_catg_column)):\n    display(df.select(non_catg_column[c]))\n  return()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b6e326d-2b2b-4917-8292-31014c1ed842"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#percentile_approx: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.percentile_approx.html\ndef countnull_for_numeric_data(df,id_lst):\n  \n  descri_table = spark.createDataFrame(df.dtypes,['col_name','data_type'])\n  descri_table.where(\"data_type = 'string' \")\n\n  #get column names by datatype\n  catg_column = [i.col_name for i in descri_table.where(\"data_type = 'string' \").collect() if i.col_name not in id_lst]\n  print(\"catg_column: \\n\", catg_column)\n\n  non_catg_column = [i.col_name for i in descri_table.where(\"data_type in ('date','bigint','int','double','float') \").collect() if i.col_name not in id_lst]\n  \n  \n  print(\"non_catg_column: \\n\",non_catg_column)\n  \n  #print('1')\n  #df_catg_all = df.groupBy(catg_column[0]).agg(countDistinct(col(id_)).alias('id_cnts'))\n  \n  \n  for c in range(len(non_catg_column)):\n    display(df.select(count(when(isnan(non_catg_column[c]) | col(non_catg_column[c]).isNull(), non_catg_column[c])).alias(non_catg_column[c])))\n  return()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"459be735-63de-4ccc-9819-8d83f62dca71"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def fillna_for_numeric_data(df,id_lst, fill_value):\n  catg_casewhen_lst = []\n  \n  descri_table = spark.createDataFrame(df.dtypes,['col_name','data_type'])\n  descri_table.where(\"data_type = 'string' \")\n\n  #get column names by datatype\n  catg_column = [i.col_name for i in descri_table.where(\"data_type = 'string' \").collect()if i.col_name not in id_lst]\n  print(\"catg_column: \\n\", catg_column)\n\n  non_catg_column = [i.col_name for i in descri_table.where(\"data_type in ('date','bigint','int','double','float') \").collect()if i.col_name not in id_lst]\n  print(\"non_catg_column: \\n\",non_catg_column)\n  \n  #print('1')\n  #df_catg_all = df.groupBy(catg_column[0]).agg(countDistinct(col(id_)).alias('id_cnts'))\n  \n  \n  df_fillna = df.fillna(fill_value, subset= non_catg_column)\n\n  return(df_fillna)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43103ce4-5847-4db0-8a5c-22e6eed60dc2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## for pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f344281-39a4-43e2-b15e-7bbe90a222d9"}}},{"cell_type":"code","source":["def convert_datatype_for_pandas(full_data):\n  #convert decimal to float\n  select_expr = [col(c).cast(\"float\") if (\"decimal\" in t) else col(c) for c, t in full_data.dtypes]\n  full_data = full_data.select(*select_expr)\n  #convert double to float\n  select_expr = [col(c).cast(\"float\") if (\"double\" in t) else col(c) for c, t in full_data.dtypes]\n  full_data = full_data.select(*select_expr)\n\n  #convert boolean to int\n  select_expr = [col(c).cast(\"int\") if (\"boolean\" in t) else col(c) for c, t in full_data.dtypes]\n  full_data = full_data.select(*select_expr)\n  return full_data.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a4247a7-edd6-4982-9eda-3230205886ff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## for model evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52520a22-86c0-48e1-8df1-96b5494f7a62"}}},{"cell_type":"code","source":["def eval_pct_qcut10(y_proba, y_test):\n  df_proba = pd.DataFrame({\"predict_proba\":y_proba[:,1], \"actual\":y_test})\n  df_proba['rank'] = df_proba['predict_proba'].rank(method='first',ascending=False)\n  df_proba['decile'] = pd.qcut(df_proba['rank'].values, 10).codes\n  df_proba = df_proba.sort_values(by=['decile'])\n  groups= df_proba.groupby(\"decile\")\n  pct = groups[\"actual\"].sum()/groups.size()\n  test_base_rate = (df_proba[\"actual\"].sum()/len(df_proba[\"actual\"]))\n  top10 = pct[0]\n  lift = top10 / test_base_rate\n  return lift, test_base_rate, pct"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"560bff75-192b-4c30-91e2-72d5c3160ede"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"common_module","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1038366931141004}},"nbformat":4,"nbformat_minor":0}
